[{"id":0,"href":"/notes/books/","title":"Books","section":"Shantanu's Notesverse","content":" Notes on books that I read # "},{"id":1,"href":"/notes/braindump/","title":"Braindump","section":"Shantanu's Notesverse","content":" Collection of random notes, thoughts, and ideas. # "},{"id":2,"href":"/notes/books/ddia/chapter-1/","title":"Chapter 1: Foundations of Data Systems","section":"Designing Data-Intensive Applications","content":" Foundations of Data Systems # 3 Importatant Characteristics of Data Systems # Reliability: System should continue to work correctly even in the face of adversity and maintain same perf levels Scalability: As the ssytem grows, there should be reasonable ways to deal with the growth Maintainability: Different people should be able to work on it productively across time Reliability # This can include:\nPerforming expected functions correctly Tolerate user mistakes or unexpected inputs Maintain good enough performance under load Prevent unauthorized access and abuse Types of faults:\nHardware -\u0026gt; Typically mitigated by redundant hardware Software -\u0026gt; Typically mitigated by thinking upfront about design and careful testing Human error -\u0026gt; Typically mitigated by careful design and testing Scalability # This is system\u0026rsquo;s ability to cope with increased load.\nLoad -\u0026gt; Can be described with the help of numbers we can call as load parameters. The parameters would depend on the system architecture. E.g.:\nWeb server -\u0026gt; Number of requests per second Database -\u0026gt; Number of read/write operations per second or read to write ratio Chat Room -\u0026gt; Simulatenously active users Cache -\u0026gt; Hit/miss rate Twitter Example # Tweet post rate -\u0026gt; 4.6K RPS average, 12K RPS peak Home TL View -\u0026gt; 300k RPS average\nOption 1:\nInsert each posted tweet to a glolable collection. When a user views their TL, fetch all their followee\u0026rsquo;s tweets and merge them. Simple but slow due to all the joins.\nOption 2:\nMaintain a cache for each user\u0026rsquo;s TL. On a tweet post, update TL cache for all followers of tweet poster. This approach is fast on read but can become very expensive for people with large following. E.g. a user with 30 mn followers would mean 30mn writes per tweet.\nFinal Solution used at twitter: Hybrid approach of above 2. For most users, do 2nd option. For users with high number of followers (e.g. celebs), do option 1 (i.e. their tweets get merged into the caches at view time).\nPerformance # Batch processing systems (e.g. Hadoop) care more about throughput while online systems care more about response time.\nLatency -\u0026gt; duration where request is waiting to be handled\nResponse time -\u0026gt; Total time spent in serving a request that a client sees (thus, including latency)\nResponse time is not a single number but a distribution since you\u0026rsquo;d get slightly different response times for every request. A good metric to use while reporting perf data like response time is percentiles, not averages. This is because averages can hide outliers and also don\u0026rsquo;t tell about how many users experienced a given response time.\nMedian (P50), P95, P99 etc are good metrics. High percentiles of response times are also called tail latencies.\nNote: Amazon describes response time requirements for internal services in terms of P99.9, even though it affects only 1 in 1000 requests. Because often those are the users with most data because they\u0026rsquo;ve made most purchases.\nAmazon observes that a 100ms increase in response time for a request reduces sales by 1%. Others report that a 1 second slow down reduces customer saitsfaction by 16%.\nNote that trying to satisfy too high tail latencies is also not userful so there should be a balance of ROI.\nQueueing delays are also known as head of line blocking.\nLoad Test -\u0026gt; Should keep generating and sending requests without waiting for previous response to finish, to simulate the behavior of a real system where there could be queueing delays.\nTail latency amplification -\u0026gt; When a user request results in muliple backend calls, even 1 slow call slows down the whole response \u0026amp; there\u0026rsquo;s a high chance of serveral users experiencing this.\nMaintainability # "}]